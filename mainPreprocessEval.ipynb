{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:22:44,240 Using CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\dalto\\Box Sync\\aaaaaaaa_s001_t000.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 323839  =      0.000 ...  1264.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 11:22:47,559 Single Data Point Test:\n",
      "2024-12-05 11:22:47,560 Actual Label: 0\n",
      "2024-12-05 11:22:47,561 Predicted Label: 2\n",
      "2024-12-05 11:22:47,562 Accuracy: 0%\n",
      "2024-12-05 11:22:47,564 Class Probabilities:\n",
      "2024-12-05 11:22:47,566 Class 0: 20.23%\n",
      "2024-12-05 11:22:47,568 Class 1: 26.36%\n",
      "2024-12-05 11:22:47,570 Class 2: 28.94%\n",
      "2024-12-05 11:22:47,571 Class 3: 24.46%\n",
      "2024-12-05 11:22:47,839 Training and evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "#this is going to be our main.py file that imports all the good preprocessing stuff and data input stuff and the machine learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import MulticlassCalibrationError  # For calibration metrics\n",
    "import torch.nn.functional as F\n",
    "import mne\n",
    "\n",
    "class Args:\n",
    "    seed_list = [0, 1004, 911, 2021, 119]\n",
    "    seed = 10\n",
    "    project_name = \"test_project\"\n",
    "    checkpoint = False\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    optim = 'adam'\n",
    "    lr_scheduler = \"Single\"\n",
    "    lr_init = 1e-3\n",
    "    lr_max = 4e-3\n",
    "    t_0 = 5\n",
    "    t_mult = 2\n",
    "    t_up = 1\n",
    "    gamma = 0.5\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-6\n",
    "    task_type = 'binary'\n",
    "    log_iter = 10\n",
    "    best = True\n",
    "    last = False\n",
    "    test_type = \"test\"\n",
    "    device = 0  # GPU device number to use\n",
    "\n",
    "    binary_target_groups = 2\n",
    "    output_dim = 4\n",
    "\n",
    "    # Manually set paths here instead of using a YAML config file\n",
    "    data_path = '/Volumes/SDCARD/v2.0.3/edf' # Set the data path directly\n",
    "    dir_root = os.getcwd()  # Set the root directory as the current working directory\n",
    "    dir_result = os.path.join(dir_root, 'results')  # Set the result directory directly\n",
    "\n",
    "    # Check if the results directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(dir_result):\n",
    "        os.makedirs(dir_result)\n",
    "    reset = False  # Set reset flag to False to avoid overwriting existing results\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_dim = 512  # Number of features in the hidden state of the LSTM\n",
    "    dropout = 0.1  # Dropout rate for regularization\n",
    "    num_channel = 20  # Number of data channels (e.g., EEG channels)\n",
    "    sincnet_bandnum = 20  # SincNet configuration\n",
    "    enc_model = 'raw'  # Encoder model for feature extraction\n",
    "\n",
    "    window_shift_label = 1\n",
    "    window_size_label = 4\n",
    "    requirement_target = None\n",
    "    sincnet_kernel_size = 81\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Shortcut for downsampling\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # First conv layer + batch norm + ReLU\n",
    "        out = self.bn2(self.conv2(out))        # Second conv layer + batch norm\n",
    "        out += self.shortcut(x)               # Add residual connection\n",
    "        out = F.relu(out)                     # ReLU activation\n",
    "        return out\n",
    "\n",
    "class CNN2D_LSTM_V8_4(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(CNN2D_LSTM_V8_4, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # Model parameters\n",
    "        self.num_layers = args.num_layers  # Number of LSTM layers\n",
    "        self.hidden_dim = 256  # Features in the LSTM hidden state\n",
    "        self.dropout = args.dropout  # Dropout rate\n",
    "        self.num_data_channel = args.num_channel  # Data channels (e.g., EEG channels)\n",
    "        self.sincnet_bandnum = args.sincnet_bandnum  # SincNet configuration\n",
    "        self.feature_extractor = args.enc_model  # Feature extraction method\n",
    "        self.in_planes = 1  # Input planes for ResNet\n",
    "\n",
    "        # Activation functions\n",
    "        activation = 'relu'  # Default activation function\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "            'relu': nn.ReLU(inplace=True),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'elu': nn.ELU()\n",
    "        })\n",
    "\n",
    "        # Initialize hidden state for LSTM\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device)\n",
    "        )\n",
    "\n",
    "        # Define ResNet layers using the BasicBlock\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "\n",
    "        # Adaptive average pooling\n",
    "        self.agvpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=64, bias=True),\n",
    "            nn.BatchNorm1d(64),\n",
    "            self.activations[activation],\n",
    "            nn.Linear(in_features=64, out_features=args.output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride1 in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride1))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Permute input\n",
    "        x = x.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "        # Pass through ResNet layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # Apply adaptive pooling\n",
    "        x = self.agvpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Initialize LSTM hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        hidden = (h0, c0)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "    def init_state(self, device):\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device)\n",
    "        )\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(labels, predictions, probabilities, num_classes):\n",
    "    # Calculate accuracy\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=list(range(num_classes)))\n",
    "\n",
    "    # Calculate precision, recall, FPR, FNR\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Precision, Recall, FPR, FNR per class\n",
    "    precision = TP / (TP + FP + 1e-8)  # Added epsilon to avoid division by zero\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    FPR = FP / (FP + TN + 1e-8)\n",
    "    FNR = FN / (FN + TP + 1e-8)\n",
    "\n",
    "    # Average metrics\n",
    "    avg_precision = np.mean(precision)\n",
    "    avg_recall = np.mean(recall)\n",
    "    avg_FPR = np.mean(FPR)\n",
    "    avg_FNR = np.mean(FNR)\n",
    "\n",
    "    # Compute calibration error\n",
    "    calibration_error = ece_criterion(probabilities, labels)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_precision': avg_precision,\n",
    "        'avg_recall': avg_recall,\n",
    "        'avg_FPR': avg_FPR,\n",
    "        'avg_FNR': avg_FNR,\n",
    "        'ECE': calibration_error.item()\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to log metrics\n",
    "def log_metrics(metrics, epoch, phase):\n",
    "    logging.info(f'{phase} Metrics after Epoch {epoch}:')\n",
    "    logging.info(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "    logging.info(f\"Precision: {metrics['avg_precision']:.4f}\")\n",
    "    logging.info(f\"Recall: {metrics['avg_recall']:.4f}\")\n",
    "    logging.info(f\"False Positive Rate: {metrics['avg_FPR']:.4f}\")\n",
    "    logging.info(f\"False Negative Rate: {metrics['avg_FNR']:.4f}\")\n",
    "    logging.info(f\"Expected Calibration Error: {metrics['ECE']:.4f}\")\n",
    "\n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar(f'{phase} accuracy', metrics['accuracy'], epoch)\n",
    "    writer.add_scalar(f'{phase} precision', metrics['avg_precision'], epoch)\n",
    "    writer.add_scalar(f'{phase} recall', metrics['avg_recall'], epoch)\n",
    "    writer.add_scalar(f'{phase} FPR', metrics['avg_FPR'], epoch)\n",
    "    writer.add_scalar(f'{phase} FNR', metrics['avg_FNR'], epoch)\n",
    "    writer.add_scalar(f'{phase} ECE', metrics['ECE'], epoch)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader, device, temperature, num_classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        all_probs = []\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Apply temperature scaling\n",
    "            scaled_outputs = outputs / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.nn.functional.softmax(scaled_outputs, dim=1)\n",
    "\n",
    "            # Get predicted labels\n",
    "            _, predicted = torch.max(probabilities.data, 1)\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predicted)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate all tensors\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_probs = torch.cat(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(all_labels, all_predictions, all_probs, num_classes)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to test on a single data point\n",
    "def test_single_data_point(model, data_point, label, device, temperature, class_names=None):\n",
    "    model.eval()\n",
    "    target_channels = 40\n",
    "    target_points = 5000\n",
    "    segment_length = 500\n",
    "    max_time_points = 100000\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # single_file_input = data_point.to(device).unsqueeze(0)  # Add batch dimension\n",
    "        data = mne.io.read_raw_edf(data_point, preload=True).get_data()\n",
    "\n",
    "        # Limit the number of time points if they exceed max_time_points\n",
    "        if data.shape[1] > max_time_points:\n",
    "            data = data[:, :max_time_points]\n",
    "\n",
    "        # Pad or trim channels to match target_channels\n",
    "        if data.shape[0] < target_channels:\n",
    "            padding = np.zeros((target_channels - data.shape[0], data.shape[1]))\n",
    "            data = np.vstack((data, padding))\n",
    "        elif data.shape[0] > target_channels:\n",
    "            data = data[:target_channels, :]\n",
    "\n",
    "        # Randomly select a segment\n",
    "        if data.shape[1] > segment_length:\n",
    "            start = np.random.randint(0, max(1, data.shape[1] - segment_length))\n",
    "            end = start + segment_length\n",
    "            segment = data[:, start:end]\n",
    "        else:\n",
    "            segment = data\n",
    "\n",
    "        # Interpolate or compress to match target_points\n",
    "        if segment.shape[1] != target_points:\n",
    "            segment = np.array([np.interp(np.linspace(0, 1, target_points),\n",
    "                                        np.linspace(0, 1, segment.shape[1]), channel)\n",
    "                                for channel in segment])\n",
    "                                \n",
    "        # Return tensor\n",
    "        data = torch.tensor(segment, dtype=torch.float32).unsqueeze(0).to(next(model.parameters()).dtype)\n",
    "        \n",
    "        output = model(data)\n",
    "        # Apply temperature scaling\n",
    "        scaled_output = output / temperature\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(scaled_output, dim=1)\n",
    "        probabilities = probabilities.cpu().numpy()[0]  # Convert to numpy array\n",
    "\n",
    "        # Get predicted label\n",
    "        predicted_label = np.argmax(probabilities)\n",
    "        actual_label = label\n",
    "\n",
    "        # Calculate accuracy (1 if correct, 0 if incorrect)\n",
    "        is_correct = int(predicted_label == actual_label)\n",
    "        accuracy = 100 * is_correct\n",
    "\n",
    "        logging.info('Single Data Point Test:')\n",
    "        if class_names:\n",
    "            logging.info(f'Actual Label: {class_names[actual_label]}')\n",
    "            logging.info(f'Predicted Label: {class_names[predicted_label]}')\n",
    "        else:\n",
    "            logging.info(f'Actual Label: {actual_label}')\n",
    "            logging.info(f'Predicted Label: {predicted_label}')\n",
    "        logging.info(f'Accuracy: {accuracy}%')\n",
    "        logging.info('Class Probabilities:')\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            class_label = class_names[i] if class_names else i\n",
    "            logging.info(f'Class {class_label}: {prob*100:.2f}%')\n",
    "            classProbs[i] = prob\n",
    "\n",
    "       \n",
    "        # Plot the probabilities as a bar graph\n",
    "        fig, ax = plt.subplots()\n",
    "        classes = class_names if class_names else list(range(len(probabilities)))\n",
    "        ax.bar(classes, probabilities)\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_title('Class Probabilities')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Add the figure to TensorBoard\n",
    "        writer.add_figure('Class Probabilities', fig)\n",
    "\n",
    "### CHANGE THIS FOR CUSTOM FILES AND MODEL PATHS ###\n",
    "model_path = r\"C:\\Users\\dalto\\Box Sync\\full_model.pth\"\n",
    "single_file = r\"C:\\Users\\dalto\\Box Sync\\aaaaaaaa_s001_t000.edf\"\n",
    "\n",
    "# List to store the probabilities of each class\n",
    "classProbs = [0,0,0,0]\n",
    "classNames = ['Healthy', 'Epilepsy', 'Stroke', 'Concussion']\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "# Device configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    logging.info(\"Using MPS device (Apple Silicon GPU).\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    logging.info(\"Using CUDA device.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    logging.info(\"Using CPU.\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "temperature = 2.0  # Temperature parameter for temperature scaling\n",
    "# checkpoint_dir = './checkpoints'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# model = CNN2D_LSTM_V8_4(args,device).to(device)  # Initialize the model\n",
    "model = torch.load(model_path, map_location=device)  # Load the state dictionary\n",
    "model = model.float()  # Convert model to float\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Calibration metric\n",
    "ece_criterion = MulticlassCalibrationError(num_classes=10, n_bins=15).to(device)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter('runs/simple_cnn')\n",
    "\n",
    "# Test on a single data point\n",
    "# Let's take the first image from the test set\n",
    "single_label = 0\n",
    "test_single_data_point(model, single_file, single_label, device, temperature)\n",
    "\n",
    "logging.info(\"Training and evaluation completed.\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
      "                                              0.0/926.4 kB ? eta -:--:--\n",
      "     ----------                             256.0/926.4 kB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  921.6/926.4 kB 14.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 926.4/926.4 kB 9.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchmetrics) (2.0.1+cu117)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0.0->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0.0->torchmetrics) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0.0->torchmetrics) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dalto\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch>=2.0.0->torchmetrics) (1.2.1)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\dalto\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchmetrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
