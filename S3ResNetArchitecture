    "# Define a basic block for ResNet. This block will be reused in the construction of ResNet layers.\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        # Input:\n",
    "        # - in_planes: Number of input channels\n",
    "        # - planes: Number of output channels after convolution\n",
    "        # - stride: Stride used in convolution to control output size\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional layer, followed by batch normalization.\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Define the second convolutional layer, with stride fixed at 1, followed by batch normalization.\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # If stride is greater than 1, downsample input to match the shape for addition.\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the BasicBlock.\n",
    "        # Input: x - input tensor\n",
    "        # Output: out - output tensor after applying convolutions and adding residual connection\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # First conv layer, followed by batch norm and ReLU activation\n",
    "        out = self.bn2(self.conv2(out))        # Second conv layer, followed by batch norm\n",
    "        out += self.shortcut(x)                # Add residual (shortcut) connection to maintain information flow\n",
    "        out = F.relu(out)                      # Apply ReLU activation again to introduce non-linearity\n",
    "        return out"
   ]
  },




    "# Manually set arguments here instead of using argparse\n",
    "class Args:\n",
    "    seed_list = [0, 1004, 911, 2021, 119]\n",
    "    seed = 10\n",
    "    project_name = \"test_project\"\n",
    "    checkpoint = False\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    optim = 'adam'\n",
    "    lr_scheduler = \"Single\"\n",
    "    lr_init = 1e-3\n",
    "    lr_max = 4e-3\n",
    "    t_0 = 5\n",
    "    t_mult = 2\n",
    "    t_up = 1\n",
    "    gamma = 0.5\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-6\n",
    "    task_type = 'binary'\n",
    "    log_iter = 10\n",
    "    best = True\n",
    "    last = False\n",
    "    test_type = \"test\"\n",
    "    device = 0  # GPU device number to use\n",
    "\n",
    "    binary_target_groups = 2\n",
    "    output_dim = 4\n",
    "\n",
    "    # Manually set paths here instead of using a YAML config file\n",
    "    data_path = '/Volumes/SDCARD/v2.0.3/edf' # Set the data path directly\n",
    "    dir_root = os.getcwd()  # Set the root directory as the current working directory\n",
    "    dir_result = os.path.join(dir_root, 'results')  # Set the result directory directly\n",
    "\n",




    "# Dynamically calculate the input size for fc1\n",
    "def get_flattened_size(model, input_shape):\n",
    "    dummy_input = torch.zeros(1, *input_shape).to(next(model.parameters()).device)\n",
    "    features = model.layer1(dummy_input)\n",
    "    features = model.layer2(features)\n",
    "    features = model.layer3(features)\n",
    "    features = model.agvpool(features)\n",
    "    return features.view(features.size(0), -1).size(1)\n",
    "\n",
    "# Define a CNN2D-LSTM model for EEG signal classification\n",
    "class CNN2D_LSTM_V8_4(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(CNN2D_LSTM_V8_4, self).__init__()      \n",
    "        self.args = args\n",
    "\n",
    "        # Set model parameters\n",
    "        self.num_layers = args.num_layers  # Number of LSTM layers\n",
    "        self.hidden_dim = 256  # Number of features in the hidden state of the LSTM\n",
    "        self.dropout = args.dropout  # Dropout rate for regularization\n",
    "        self.num_data_channel = args.num_channel  # Number of data channels (e.g., EEG channels)\n",
    "        self.sincnet_bandnum = args.sincnet_bandnum  # SincNet configuration\n",
    "        self.feature_extractor = args.enc_model  # Feature extraction method\n",
    "        self.in_planes = 1  # Initial number of input planes for ResNet (matching the number of input channels)\n",
    "\n",
    "        # Activation functions\n",
    "        activation = 'relu'  # Use ReLU as the default activation function\n",
    "        self.activations = nn.ModuleDict([\n",
    "            ['lrelu', nn.LeakyReLU()],\n",
    "            ['prelu', nn.PReLU()],\n",
    "            ['relu', nn.ReLU(inplace=True)],\n",
    "            ['tanh', nn.Tanh()],\n",
    "            ['sigmoid', nn.Sigmoid()],\n",
    "            ['leaky_relu', nn.LeakyReLU(0.2)],\n",
    "            ['elu', nn.ELU()]\n",
    "        ])\n",
    "\n",
    "        # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "        self.hidden = (\n",
    "            (torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device),\n",
    "             torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device))\n",
    "        )\n",
    "\n",
    "        # Define the ResNet layers using the BasicBlock\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)  # First ResNet layer with 64 output channels\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)  # Second ResNet layer with 128 output channels\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)  # Third ResNet layer with 256 output channels\n",
    "\n",
    "        # Adaptive average pooling to reduce spatial dimensions to (1, 1)\n",
    "        self.agvpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # LSTM layer for temporal sequence learning\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,  # Input size matches the output of the ResNet layers\n",
    "            hidden_size=self.hidden_dim,  # Number of features in LSTM hidden state\n",
    "            num_layers=args.num_layers,  # Number of LSTM layers\n",
    "            batch_first=True,  # Input and output tensors are provided as (batch, seq, feature)\n",
    "            dropout=args.dropout  # Dropout for regularization\n",
    "        )\n",
    "\n",
    "        # Fully connected classifier layer for outputting class probabilities\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=64, bias=True),  # Linear layer to reduce feature dimension\n",
    "            nn.BatchNorm1d(64),  # Batch normalization layer\n",
    "            self.activations[activation],  # Activation function\n",
    "            nn.Linear(in_features=64, out_features=args.output_dim, bias=True),  # Final linear layer for classification\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # Create a ResNet layer with multiple blocks.\n",
    "        # Input:\n",
    "        # - block: Block type (BasicBlock)\n",
    "        # - planes: Number of output channels for this layer\n",
    "        # - num_blocks: Number of blocks in this layer\n",
    "        # - stride: Stride for the first block\n",
    "        strides = [stride] + [1] * (num_blocks - 1)  # Set stride for the first block, others have stride of 1\n",
    "        layers = []\n",
    "        for stride1 in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride1))  # Append blocks to the layer\n",
    "            self.in_planes = planes  # Update input channel size for the next block\n",
    "        return nn.Sequential(*layers)  # Return the complete layer as a sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the CNN2D-LSTM model.\n",
    "        # Input: x - input tensor of shape (batch_size, channels, sequence_length)\n",
    "        # Output: output - output tensor with class scores\n",
    "\n",
    "        batch_size = x.size(0)  # Get the batch size from the input tensor\n",
    "\n",
    "        # Permute the input to (batch_size, sequence_length, channels)\n",
    "        x = x.permute(0, 2, 1)  # You may need to adjust this permutation based on your input shape\n",
    "\n",
    "        # Reshape to add channel dimension (e.g., for convolutional layers)\n",
    "        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length, channels)\n",
    "\n",
    "        # Pass through ResNet layers\n",
    "        x = self.layer1(x)  # Pass through first ResNet layer\n",
    "        x = self.layer2(x)  # Pass through second ResNet layer\n",
    "        x = self.layer3(x)  # Pass through third ResNet layer\n",
    "\n",
    "        # Apply adaptive average pooling to reduce spatial size to (1, 1)\n",
    "        x = self.agvpool(x)  # Shape: (batch_size, features, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to (batch_size, features)\n",
    "\n",
    "        # Prepare data for LSTM\n",
    "        # Assuming that the LSTM expects (batch_size, seq_len, input_size)\n",
    "        x = x.unsqueeze(1)  # Add a time dimension to make it compatible with LSTM (batch_size, seq_len=1, features)\n",
    "\n",
    "        # Initialize the hidden state for the LSTM with the correct batch size\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
    "        hidden = (h0, c0)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(x, hidden)  # Apply LSTM to learn temporal dependencies\n",
    "        output = output[:, -1, :]  # Take the output from the last time step of the sequence\n",
    "\n",
    "        # Classification through the fully connected layer\n",
    "        output = self.classifier(output)  # Classify using the fully connected layer\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_state(self, device):\n",
    "        # Initialize the hidden state for the LSTM\n",
    "        # Input: device - The device (CPU or GPU) where the hidden state should be allocated\n",
    "        # Output: Initializes the hidden state with zeros\n",
    "        self.hidden = (\n",
    "            (torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device),\n",
    "             torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 49 is out of bounds for dimension 0 with size 40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     25\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     27\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     28\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[174], line 29\u001b[0m, in \u001b[0;36mEEGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     28\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meeg_file_paths[idx]\n\u001b[0;32m---> 29\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Fix the file path if it contains '_%02d_%02d' pattern\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     s3_path \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, file_path)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 49 is out of bounds for dimension 0 with size 40"
     ]
    }
   ],
   "source": [
    "# Dynamically calculate the input size for fc1\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = CNN2D_LSTM_V8_4(args, device)\n",
    "\n",
    "input_channels = 1\n",
    "\n",
    "# Calculate the flattened size\n",
    "flattened_size = get_flattened_size(model, (input_channels, 40, 5000))\n",
    "\n",
    "# Update the model's fully connected layer to use the correct input size\n",
    "model.fc1 = nn.Linear(flattened_size, 128)\n",
    "\n",
    "# Cell 6: Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Cell 7: Training loop with batch processing\n",
    "num_epochs = 2  # Set the number of epochs\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (batch, labels) in enumerate(dataloader):\n",
    "        inputs = batch\n",
    "        labels = labels\n",
    "        \n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for evaluation\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Clear unused variables to free memory\n",
    "        del inputs, labels, outputs, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "y_true = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2500\n",
      "Confusion Matrix:\n",
      "[[13 60 27  0]\n",
      " [16 69 13  2]\n",
      " [ 4 84 12  0]\n",
      " [ 5  0  7  0]]\n",
      "Accuracy: 0.2500\n",
      "Sensitivity (Recall for Positive Class): 0.3013\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1583: UserWarning: Note that pos_label (set to 2) is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sensitivity = recall_score(y_true, y_pred, pos_label=2, average='weighted')  # True Positive Rate (Recall)\n",
    "# specificity = recall_score(y_true, y_pred, pos_label=2, average='weighted')  # True Negative Rate\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "# print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Update xticks and yticks for 4 classes\n",
    "classes = ['Healthy', 'Epilepsy', 'Stroke', 'Concussion']\n",
    "plt.xticks(range(4), classes)\n",
    "plt.yticks(range(4), classes)\n",
    "\n",
    "# Show the values in the heatmap\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 197\u001b[0m\n\u001b[1;32m    194\u001b[0m         save_valid_results\u001b[38;5;241m.\u001b[39mresults_all_seeds(logger\u001b[38;5;241m.\u001b[39mtest_results)  \u001b[38;5;66;03m# Save validation results\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 172\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m set_seeds(args)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Initialize the logger to track training and validation metrics\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Logger instance to save metrics\u001b[39;00m\n\u001b[1;32m    173\u001b[0m logger\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mbest_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Load preprocessed data (train, validation, test)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m, in \u001b[0;36mLogger.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_save \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241m.\u001b[39mdeepcopy(args)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Evaluator\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;241m=\u001b[39m Evaluator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# Manually set arguments here instead of using argparse\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class Args:\n",
    "    seed_list = [0, 1004, 911, 2021, 119]\n",
    "    seed = 10\n",
    "    project_name = \"test_project\"\n",
    "    checkpoint = False\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    optim = 'adam'\n",
    "    lr_scheduler = \"Single\"\n",
    "    lr_init = 1e-3\n",
    "    lr_max = 4e-3\n",
    "    t_0 = 5\n",
    "    t_mult = 2\n",
    "    t_up = 1\n",
    "    gamma = 0.5\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-6\n",
    "    task_type = 'binary'\n",
    "    log_iter = 10\n",
    "    best = True\n",
    "    last = False\n",
    "    test_type = \"test\"\n",
    "    device = 0  # GPU device number to use\n",
    "\n",
    "    binary_target_groups = 2\n",
    "    output_dim = 2\n",
    "\n",
    "    dir_root = os.getcwd()  # Set the root directory as the current working directory\n",
    "    dir_result = os.path.join(dir_root, 'results')  # Set the result directory directly\n",
    "\n",
    "    # Check if the results directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(dir_result):\n",
    "        os.makedirs(dir_result)\n",
    "    reset = False  # Set reset flag to False to avoid overwriting existing results\n",
    "\n",
    "    num_layers = 2\n",
    "    hidden_dim = 512  # Number of features in the hidden state of the LSTM\n",
    "    dropout = 0.1  # Dropout rate for regularization\n",
    "    num_channel = 20  # Number of data channels (e.g., EEG channels)\n",
    "    sincnet_bandnum = 20  # SincNet configuration\n",
    "    enc_model = 'raw'  # Encoder model for feature extraction\n",
    "\n",
    "    window_shift_label = 1\n",
    "    window_size_label = 4\n",
    "    requirement_target = None\n",
    "    sincnet_kernel_size = 81\n",
    "\n",
    "def initialize_model(args, device):\n",
    "    # Create the model\n",
    "    model = CNN2D_LSTM_V8_4(args, device).to(device)  # Directly initialize ResNetLSTM and move to the appropriate device (CPU, GPU, or MPS)\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(args, model, device, logger, seed_num):\n",
    "    # Load checkpoint if specified\n",
    "    if args.checkpoint:\n",
    "        if args.last:\n",
    "            ckpt_path = args.dir_result + '/' + args.project_name + '/ckpts/best_{}.pth'.format(str(seed_num))\n",
    "        elif args.best:\n",
    "            ckpt_path = args.dir_result + '/' + args.project_name + '/ckpts/best_{}.pth'.format(str(seed_num))\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)  # Load model checkpoint from file\n",
    "        model.load_state_dict(checkpoint['model'])  # Load saved model state\n",
    "        logger.best_auc = checkpoint['score']  # Set best AUC score from checkpoint\n",
    "        start_epoch = checkpoint['epoch']  # Set starting epoch from checkpoint\n",
    "        del checkpoint\n",
    "    else:\n",
    "        logger.best_auc = 0\n",
    "        start_epoch = 1\n",
    "    return model, start_epoch\n",
    "\n",
    "def set_optimizer(args, model):\n",
    "    # Set up the optimizer based on specified argument\n",
    "    if args.optim == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr_init, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr_init, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=args.lr_init, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'adam_lars':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr_init, weight_decay=args.weight_decay)\n",
    "        optimizer = LARC(optimizer=optimizer, eps=1e-8, trust_coefficient=0.001)  # LARS wrapper for adaptive learning rate scaling\n",
    "    elif args.optim == 'sgd_lars':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr_init, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "        optimizer = LARC(optimizer=optimizer, eps=1e-8, trust_coefficient=0.001)\n",
    "    elif args.optim == 'adamw_lars':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=args.lr_init, weight_decay=args.weight_decay)\n",
    "        optimizer = LARC(optimizer=optimizer, eps=1e-8, trust_coefficient=0.001)\n",
    "    return optimizer\n",
    "\n",
    "def set_scheduler(args, optimizer, one_epoch_iter_num):\n",
    "    # Set up learning rate scheduler\n",
    "    if args.lr_scheduler == \"CosineAnnealing\":\n",
    "        scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=args.t_0 * one_epoch_iter_num, T_mult=args.t_mult, eta_max=args.lr_max, T_up=args.t_up * one_epoch_iter_num, gamma=args.gamma)  # Custom cosine annealing scheduler with warmup (external file needed)\n",
    "    elif args.lr_scheduler == \"Single\":\n",
    "        scheduler = CosineAnnealingWarmUpSingle(optimizer, max_lr=args.lr_init * math.sqrt(args.batch_size), epochs=args.epochs, steps_per_epoch=one_epoch_iter_num, div_factor=math.sqrt(args.batch_size))  # Alternative scheduler (external file needed)\n",
    "    return scheduler\n",
    "\n",
    "def train_model(args, model, train_loader, val_loader, device, logger, optimizer, scheduler, start_epoch):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    logger.loss = 0\n",
    "\n",
    "    # Start training process\n",
    "    start = time.time()  # Start time for tracking training duration\n",
    "    pbar = tqdm(total=args.epochs, initial=0, bar_format=\"{desc:<5}{percentage:3.0f}%|{bar:10}{r_bar}\")  # Progress bar for tracking training progress\n",
    "    for epoch in range(start_epoch, args.epochs + 1):\n",
    "        epoch_losses = []\n",
    "        loss = 0\n",
    "\n",
    "        # Iterate through each batch of training data\n",
    "        for train_batch in train_loader:\n",
    "            train_x, train_y, signal_name_list = train_batch  # Unpack training batch\n",
    "            target_lengths = 500\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)  # Move data to appropriate device (CPU, GPU, or MPS)\n",
    "            iteration += 1\n",
    "\n",
    "            # Train the model and get loss\n",
    "            model, iter_loss = get_trainer(args, iteration, train_x, train_y, target_lengths, model, logger, device, scheduler, optimizer, nn.CrossEntropyLoss(reduction='none'), signal_name_list)  # Perform a training step (external file needed)\n",
    "            logger.loss += np.mean(iter_loss)\n",
    "\n",
    "            # Logging training progress\n",
    "            if iteration % args.log_iter == 0:\n",
    "                logger.log_tqdm(epoch, iteration, pbar)  # Log progress in tqdm\n",
    "                logger.log_scalars(iteration)  # Log scalar metrics (e.g., loss)\n",
    "\n",
    "            # Validation at intervals during training\n",
    "            if iteration % (len(train_loader) // 10) == 0:\n",
    "                validate_model(args, model, val_loader, device, logger, iteration, scheduler, optimizer)\n",
    "        pbar.update(1)\n",
    "    return model\n",
    "\n",
    "def validate_model(args, model, val_loader, device, logger, iteration, scheduler, optimizer):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    logger.evaluator.reset()  # Reset evaluator for validation\n",
    "    val_iteration = 0\n",
    "    logger.val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(val_loader)):\n",
    "            val_x, val_y, seq_lengths, target_lengths, aug_list, signal_name_list = batch  # Unpack validation batch\n",
    "            val_x, val_y = val_x.to(device), val_y.to(device)  # Move data to appropriate device (CPU, GPU, or MPS)\n",
    "            model, val_loss = get_trainer(args, iteration, val_x, val_y, seq_lengths, target_lengths, model, logger, device, scheduler, optimizer, nn.CrossEntropyLoss(reduction='none'), signal_name_list, flow_type=args.test_type)  # Perform validation step\n",
    "            logger.val_loss += np.mean(val_loss)\n",
    "            val_iteration += 1\n",
    "        \n",
    "        logger.log_val_loss(val_iteration, iteration)  # Log validation loss\n",
    "        logger.add_validation_logs(iteration)  # Add validation metrics to log\n",
    "        logger.save(model, optimizer, iteration, iteration)  # Save model checkpoint\n",
    "    model.train()  # Set model back to training mode\n",
    "\n",
    "def main():\n",
    "    args = Args()\n",
    "\n",
    "    # Set the device (MPS for Apple silicon, CUDA for Nvidia GPUs, or CPU)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define result classes for validation and test results\n",
    "    #save_valid_results = experiment_results_validation(args)\n",
    "    #save_test_results = experiment_results(args)\n",
    "\n",
    "    # Loop through each seed to train and evaluate the model\n",
    "    for seed_num in args.seed_list:\n",
    "        # Set the seed for reproducibility\n",
    "        args.seed = seed_num\n",
    "        set_seeds(args)\n",
    "\n",
    "        # Initialize the logger to track training and validation metrics\n",
    "        logger = Logger(args)  # Logger instance to save metrics\n",
    "        logger.evaluator.best_auc = 0\n",
    "\n",
    "        # Load preprocessed data (train, validation, test)\n",
    "        train_loader, val_loader, test_loader, len_train_dir, len_val_dir, len_test_dir = get_data_preprocessed(args)  # Data loaders for training, validation, and testing\n",
    "\n",
    "        # Initialize model\n",
    "        model = initialize_model(args, device)\n",
    "\n",
    "        # Load checkpoint if available\n",
    "        model, start_epoch = load_checkpoint(args, model, device, logger, seed_num)\n",
    "\n",
    "        # Set up optimizer and scheduler\n",
    "        optimizer = set_optimizer(args, model)\n",
    "        one_epoch_iter_num = len(train_loader)  # Total number of iterations per epoch\n",
    "        scheduler = set_scheduler(args, optimizer, one_epoch_iter_num)\n",
    "\n",
    "        # Train model\n",
    "        model = train_model(args, model, train_loader, val_loader, device, logger, optimizer, scheduler, start_epoch)\n",
    "\n",
    "        # Log validation results\n",
    "        logger.val_result_only()  # Log only validation results\n",
    "        save_valid_results.results_all_seeds(logger.test_results)  # Save validation results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "# Function for bandpass filtering\n",
    "def BPFilter(RawEEGDataFile):\n",
    "    BPEEGDataFile = RawEEGDataFile.copy().filter(l_freq=0.5, h_freq=20.0, fir_design='firwin')\n",
    "    return BPEEGDataFile\n",
    "\n",
    "# Function to load and preprocess each .edf file from S3\n",
    "def load_and_preprocess_edf(s3_path, target_channels=40, max_points=75000):\n",
    "    # Initialize S3 filesystem\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    # Load the raw EEG data directly from S3\n",
    "    with fs.open(s3_path, 'rb') as f:\n",
    "        # Read the data into memory\n",
    "        edf_bytes = f.read()\n",
    "\n",
    "    # Write the bytes to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".edf\", delete=True) as tmp:\n",
    "        tmp.write(edf_bytes)\n",
    "        tmp.flush()  # Ensure all data is written\n",
    "\n",
    "        # Load the raw EEG data from the temporary file\n",
    "        RawEEGDataFile = mne.io.read_raw_edf(tmp.name, preload=True, verbose=False)\n",
    "\n",
    "    # Interpolate bad channels\n",
    "    RawEEGDataFile.interpolate_bads()\n",
    "\n",
    "    # Determine the current number of time points and sampling frequency\n",
    "    current_points = len(RawEEGDataFile.times)\n",
    "    current_sfreq = RawEEGDataFile.info['sfreq']\n",
    "\n",
    "    # Calculate the new sampling frequency required to get exactly max_points\n",
    "    if current_points != max_points:\n",
    "        new_sfreq = (max_points / current_points) * current_sfreq\n",
    "        # Resample the data to achieve exactly max_points\n",
    "        RawEEGDataFile.resample(new_sfreq, npad=\"auto\")\n",
    "\n",
    "    # Apply bandpass filter to the EEG data\n",
    "    BPEEGDataFile = BPFilter(RawEEGDataFile)\n",
    "\n",
    "    # Get the raw data (channels  time)\n",
    "    data = BPEEGDataFile.get_data()\n",
    "\n",
    "    # Ensure the data has exactly max_points (75000)\n",
    "    if data.shape[1] != max_points:\n",
    "        # Truncate or pad the data to have max_points time samples\n",
    "        if data.shape[1] > max_points:\n",
    "            data = data[:, :max_points]\n",
    "        else:\n",
    "            padding = max_points - data.shape[1]\n",
    "            data = np.pad(data, ((0, 0), (0, padding)), mode='constant')\n",
    "\n",
    "    # Determine current number of channels\n",
    "    current_channels, time_points = data.shape\n",
    "\n",
    "    # Pad or truncate channels to make them equal to target_channels (e.g., 40)\n",
    "    if current_channels < target_channels:\n",
    "        # Pad with zeros if there are fewer channels than target_channels\n",
    "        padding = target_channels - current_channels\n",
    "        padded_data = np.pad(data, ((0, padding), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        # Truncate channels if there are more than target_channels\n",
    "        padded_data = data[:target_channels, :]\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "# Load and preprocess all the files from S3\n",
    "eeg_data = []\n",
    "count = 0\n",
    "for path in matching_df['s3_path']:\n",
    "    if count < 3:  # Limit to 3 files for demonstration\n",
    "        subData = load_and_preprocess_edf(path)\n",
    "        eeg_data.append(subData)\n",
    "        print(f\"Loaded {path}, Shape: {subData.shape}\")\n",
    "        count += 1\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "eeg_data = np.array(eeg_data)\n",
    "\n",
    "# Reshape data to fit ResNet input shape (assuming 1D signals, we can reshape to (samples, height, width, channels))\n",
    "eeg_data = np.expand_dims(eeg_data, axis=-1)  # Add channel dimension\n",
    "\n",
    "print(\"EEG data shape:\", eeg_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for binary classification...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m             segment \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtarget_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtarget_points), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m segment, label, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(data_path)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m train_loader, val_loader, test_loader, len_train_dir, len_val_dir, len_test_dir \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_preprocessed\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Set mne logging level to suppress excessive output\u001b[39;00m\n\u001b[1;32m     68\u001b[0m mne\u001b[38;5;241m.\u001b[39mset_log_level(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWARNING\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[111], line 55\u001b[0m, in \u001b[0;36mget_data_preprocessed\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing for binary classification...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m---> 55\u001b[0m mlData \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Extract file paths and labels from mlData\u001b[39;00m\n\u001b[1;32m     58\u001b[0m edf_files \u001b[38;5;241m=\u001b[39m mlData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[111], line 120\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Read labels\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     labelFrame \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is label frame:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, labelFrame)\n\u001b[1;32m    123\u001b[0m     labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Detector_Dataset Class with integrated preprocessing\n",
    "class Detector_Dataset(Dataset):\n",
    "    def __init__(self, data_paths, labels, args, augment=None, data_type=None):\n",
    "        self.data_paths = data_paths  # Paths to the raw data files\n",
    "        self.labels = labels  # Corresponding labels for each data file\n",
    "        self.args = args\n",
    "        self.augment = augment\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        try:\n",
    "            # Optionally print the data path being processed\n",
    "            if self.args.verbose and index % 10 == 0:\n",
    "                print(f\"Processing file {index}: {data_path}\")\n",
    "\n",
    "            # Process the EDF file using the preprocessing functions\n",
    "            BPEEGDataFile, _ = EDFProcess(data_path)\n",
    "\n",
    "            # Get the preprocessed data\n",
    "            data = BPEEGDataFile.get_data()\n",
    "\n",
    "            # Limit the number of time points if they exceed max_time_points\n",
    "            if data.shape[1] > self.args.max_time_points:\n",
    "                data = data[:, :self.args.max_time_points]\n",
    "\n",
    "            # Pad or trim channels to match target_channels\n",
    "            if data.shape[0] < self.args.target_channels:\n",
    "                padding = np.zeros((self.args.target_channels - data.shape[0], data.shape[1]))\n",
    "                data = np.vstack((data, padding))\n",
    "            elif data.shape[0] > self.args.target_channels:\n",
    "                data = data[:self.args.target_channels, :]\n",
    "\n",
    "            # Randomly select a segment\n",
    "            if data.shape[1] > self.args.segment_length:\n",
    "                start = np.random.randint(0, max(1, data.shape[1] - self.args.segment_length))\n",
    "                end = start + self.args.segment_length\n",
    "                segment = data[:, start:end]\n",
    "            else:\n",
    "                segment = data\n",
    "\n",
    "            # Interpolate or compress each segment to match target_points\n",
    "            if segment.shape[1] != self.args.target_points:\n",
    "                segment = np.array([np.interp(np.linspace(0, 1, self.args.target_points),\n",
    "                                              np.linspace(0, 1, segment.shape[1]), channel)\n",
    "                                    for channel in segment])\n",
    "\n",
    "            # Convert to tensor\n",
    "            segment = torch.tensor(np.expand_dims(segment, axis=0), dtype=torch.float32)\n",
    "\n",
    "            return segment, label, os.path.basename(data_path).split(\".\")[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {data_path}: {e}\")\n",
    "            # Return a zero tensor if there's an error\n",
    "            segment = torch.zeros((1, self.args.target_channels, self.args.target_points), dtype=torch.float32)\n",
    "            return segment, label, os.path.basename(data_path).split(\".\")[0]\n",
    "        \n",
    "train_loader, val_loader, test_loader, len_train_dir, len_val_dir, len_test_dir = get_data_preprocessed(args)\n",
    "\n",
    "# Set mne logging level to suppress excessive output\n",
    "mne.set_log_level(verbose='WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic block for ResNet. This block will be reused in the construction of ResNet layers.\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        # Input:\n",
    "        # - in_planes: Number of input channels\n",
    "        # - planes: Number of output channels after convolution\n",
    "        # - stride: Stride used in convolution to control output size\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional layer, followed by batch normalization.\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Define the second convolutional layer, with stride fixed at 1, followed by batch normalization.\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # If stride is greater than 1, downsample input to match the shape for addition.\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the BasicBlock.\n",
    "        # Input: x - input tensor\n",
    "        # Output: out - output tensor after applying convolutions and adding residual connection\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # First conv layer, followed by batch norm and ReLU activation\n",
    "        out = self.bn2(self.conv2(out))        # Second conv layer, followed by batch norm\n",
    "        out += self.shortcut(x)                # Add residual (shortcut) connection to maintain information flow\n",
    "        out = F.relu(out)                      # Apply ReLU activation again to introduce non-linearity\n",
    "        return out\n",
    "\n",
    "# Define a CNN2D-LSTM model for EEG signal classification\n",
    "class CNN2D_LSTM_V8_4(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        # Input:\n",
    "        # - args: Arguments containing model configurations like number of layers, dropout, etc.\n",
    "        # - device: Device where the model will be run (e.g., CPU or GPU)\n",
    "        super(CNN2D_LSTM_V8_4, self).__init__()      \n",
    "        self.args = args\n",
    "\n",
    "        # Set model parameters\n",
    "        self.num_layers = args.num_layers  # Number of LSTM layers\n",
    "        self.hidden_dim = 256  # Number of features in the hidden state of the LSTM\n",
    "        self.dropout = args.dropout  # Dropout rate for regularization\n",
    "        self.num_data_channel = args.num_channel  # Number of data channels (e.g., EEG channels)\n",
    "        self.sincnet_bandnum = args.sincnet_bandnum  # SincNet configuration\n",
    "        self.feature_extractor = args.enc_model  # Feature extraction method\n",
    "\n",
    "        # Initialize feature extraction model based on selected type\n",
    "        if self.feature_extractor == \"raw\" or self.feature_extractor == \"downsampled\":\n",
    "            pass  # No additional feature extraction needed for raw or downsampled data\n",
    "        else:\n",
    "            # Define feature extraction models using a dictionary\n",
    "            self.feat_models = nn.ModuleDict([\n",
    "                ['psd1', PSD_FEATURE1()],\n",
    "                ['psd2', PSD_FEATURE2()],\n",
    "                ['stft1', SPECTROGRAM_FEATURE_BINARY1()],\n",
    "                ['stft2', SPECTROGRAM_FEATURE_BINARY2()],\n",
    "                ['LFCC', LFCC_FEATURE()],                                \n",
    "                ['sincnet', SINCNET_FEATURE(args=args, num_eeg_channel=self.num_data_channel)]  # SincNet feature extractor\n",
    "            ])\n",
    "            self.feat_model = self.feat_models[self.feature_extractor]  # Select the appropriate feature extractor\n",
    "\n",
    "        # Determine the number of features for each feature extractor\n",
    "        if args.enc_model == \"psd1\" or args.enc_model == \"psd2\":\n",
    "            self.feature_num = 7  # PSD feature extractor outputs 7 features\n",
    "        elif args.enc_model == \"sincnet\":\n",
    "            self.feature_num = args.cnn_channel_sizes[args.sincnet_layer_num-1]  # SincNet output depends on channel size\n",
    "        elif args.enc_model == \"stft1\":\n",
    "            self.feature_num = 50  # STFT1 outputs 50 features\n",
    "        elif args.enc_model == \"stft2\":\n",
    "            self.feature_num = 100  # STFT2 outputs 100 features\n",
    "        elif args.enc_model == \"raw\":\n",
    "            self.feature_num = 1  # Raw input has only one feature channel\n",
    "            self.num_data_channel = 1  # Set the number of data channels to 1 for raw input\n",
    "        self.in_planes = 64  # Initial number of input planes for ResNet\n",
    "\n",
    "        # Activation functions\n",
    "        activation = 'relu'  # Use ReLU as the default activation function\n",
    "        self.activations = nn.ModuleDict([\n",
    "            ['lrelu', nn.LeakyReLU()],\n",
    "            ['prelu', nn.PReLU()],\n",
    "            ['relu', nn.ReLU(inplace=True)],\n",
    "            ['tanh', nn.Tanh()],\n",
    "            ['sigmoid', nn.Sigmoid()],\n",
    "            ['leaky_relu', nn.LeakyReLU(0.2)],\n",
    "            ['elu', nn.ELU()]\n",
    "        ])\n",
    "\n",
    "        # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "        self.hidden = (\n",
    "            (torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device),\n",
    "             torch.zeros(self.num_layers, args.batch_size, self.hidden_dim).to(device))\n",
    "        )\n",
    "\n",
    "        # Define helper functions for convolutional layers with batch normalization and activation\n",
    "        def conv2d_bn(inp, oup, kernel_size, stride, padding, dilation=1):\n",
    "            # Convolutional layer followed by batch normalization and activation\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                self.activations[activation],\n",
    "            )\n",
    "\n",
    "        def conv2d_bn_nodr(inp, oup, kernel_size, stride, padding):\n",
    "            # Convolutional layer followed by batch normalization and activation (no dilation)\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                self.activations[activation],\n",
    "            )  \n",
    "\n",
    "        # Define the first convolutional layer and pooling based on the feature extraction method\n",
    "        if args.enc_model == \"raw\":\n",
    "            self.conv1 = conv2d_bn(self.num_data_channel,  64, (1, 51), (1, 4), (0, 25))  # Raw data convolution\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))  # Max pooling to reduce temporal dimension\n",
    "        elif args.enc_model == \"sincnet\":\n",
    "            self.conv1 = conv2d_bn(1,  64, (7, 21), (7, 2), (0, 10))  # SincNet feature convolution\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))  # Max pooling\n",
    "        elif args.enc_model == \"psd1\" or args.enc_model == \"psd2\" or args.enc_model == \"stft2\":\n",
    "            self.conv1 = conv2d_bn(1,  64, (7, 21), (7, 2), (0, 10))  # PSD or STFT2 feature convolution\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))  # Max pooling\n",
    "        elif args.enc_model == \"LFCC\":\n",
    "            self.conv1 = conv2d_bn(1,  64, (8, 21), (8, 2), (0, 10))  # LFCC feature convolution\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))  # Max pooling\n",
    "        elif args.enc_model == \"downsampled\":\n",
    "            # Define multiple convolutional layers for downsampled data at different frequencies\n",
    "            self.conv2d_200hz = conv2d_bn_nodr(1,  32, (1, 51), (1, 4), (0, 25))  # 200 Hz convolution\n",
    "            self.conv2d_100hz = conv2d_bn_nodr(1,  16, (1, 51), (1, 2), (0, 25))  # 100 Hz convolution\n",
    "            self.conv2d_50hz = conv2d_bn_nodr(1,  16, (1, 51), (1, 1), (0, 25))  # 50 Hz convolution\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))  # Max pooling\n",
    "\n",
    "        # Define the ResNet layers using the BasicBlock\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)  # First ResNet layer with 64 output channels\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)  # Second ResNet layer with 128 output channels\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)  # Third ResNet layer with 256 output channels\n",
    "\n",
    "        # Adaptive average pooling to reduce spatial dimensions to (1, 1)\n",
    "        self.agvpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # LSTM layer for temporal sequence learning\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,  # Input size matches the output of the ResNet layers\n",
    "            hidden_size=self.hidden_dim,  # Number of features in LSTM hidden state\n",
    "            num_layers=args.num_layers,  # Number of LSTM layers\n",
    "            batch_first=True,  # Input and output tensors are provided as (batch, seq, feature)\n",
    "            dropout=args.dropout  # Dropout for regularization\n",
    "        )\n",
    "\n",
    "        # Fully connected classifier layer for outputting class probabilities\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=64, bias=True),  # Linear layer to reduce feature dimension\n",
    "            nn.BatchNorm1d(64),  # Batch normalization layer\n",
    "            self.activations[activation],  # Activation function\n",
    "            nn.Linear(in_features=64, out_features=args.output_dim, bias=True),  # Final linear layer for classification\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # Create a ResNet layer with multiple blocks.\n",
    "        # Input:\n",
    "        # - block: Block type (BasicBlock)\n",
    "        # - planes: Number of output channels for this layer\n",
    "        # - num_blocks: Number of blocks in this layer\n",
    "        # - stride: Stride for the first block\n",
    "        strides = [stride] + [1] * (num_blocks - 1)  # Set stride for the first block, others have stride of 1\n",
    "        layers = []\n",
    "        for stride1 in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride1))  # Append blocks to the layer\n",
    "            self.in_planes = planes  # Update input channel size for the next block\n",
    "        return nn.Sequential(*layers)  # Return the complete layer as a sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the CNN2D-LSTM model.\n",
    "        # Input: x - input tensor of shape (batch_size, channels, sequence_length)\n",
    "        # Output: output - output tensor with class scores\n",
    "        x = x.permute(0, 2, 1)  # Permute the input to (batch_size, channels, sequence_length)\n",
    "\n",
    "        # Apply different feature extraction methods based on the selected type\n",
    "        if self.feature_extractor == \"downsampled\":\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension\n",
    "            x_200 = self.conv2d_200hz(x)  # Apply 200 Hz convolution\n",
    "            x_100 = self.conv2d_100hz(x[:, :, :, ::2])  # Apply 100 Hz convolution to downsampled data\n",
    "            x_50 = self.conv2d_50hz(x[:, :, :, ::4])  # Apply 50 Hz convolution to further downsampled data\n",
    "            x = torch.cat((x_200, x_100, x_50), dim=1)  # Concatenate outputs along the channel dimension\n",
    "            x = self.maxpool1(x)  # Apply max pooling to reduce the spatial dimensions\n",
    "        elif self.feature_extractor != \"raw\":\n",
    "            x = self.feat_model(x)  # Extract features using the selected feature extraction model\n",
    "            x = x.reshape(x.size(0), -1, x.size(3)).unsqueeze(1)  # Reshape and add a channel dimension\n",
    "            x = self.conv1(x)  # Apply first convolution\n",
    "            x = self.maxpool1(x)  # Apply max pooling\n",
    "        else:\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension for raw input\n",
    "            x = self.conv1(x)  # Apply first convolution\n",
    "            x = self.maxpool1(x)  # Apply max pooling\n",
    "\n",
    "        # Pass through ResNet layers\n",
    "        x = self.layer1(x)  # Pass through first ResNet layer\n",
    "        x = self.layer2(x)  # Pass through second ResNet layer\n",
    "        x = self.layer3(x)  # Pass through third ResNet layer\n",
    "        x = self.agvpool(x)  # Apply adaptive average pooling to reduce spatial size to (1, 1)\n",
    "        x = torch.squeeze(x, 2)  # Squeeze the height dimension to make it compatible for LSTM\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, sequence_length, features)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        self.hidden = tuple(([Variable(var.data) for var in self.hidden]))  # Update the hidden state with current values\n",
    "        output, self.hidden = self.lstm(x, self.hidden)  # Apply LSTM to learn temporal dependencies\n",
    "        output = output[:, -1, :]  # Take the output from the last time step of the sequence\n",
    "        output = self.classifier(output)  # Classify using the fully connected layer\n",
    "        return output, self.hidden  # Output the classification results and the hidden state\n",
    "\n",
    "    def init_state(self, device):\n",
    "        # Initialize the hidden state for the LSTM\n",
    "        # Input: device - The device (CPU or GPU) where the hidden state should be allocated\n",
    "        # Output: Initializes the hidden state with zeros\n",
    "        self.hidden = (\n",
    "            (torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device),\n",
    "             torch.zeros(self.num_layers, self.args.batch_size, self.hidden_dim).to(device))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(matching_labels)\n",
    "categorical_labels = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(eeg_data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Data is (batch_size, num_channels, signal_length, 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Reshape to (batch_size, 1, num_channels, signal_length)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatching_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Labels as long tensor\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a dataset and dataloader\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(data, labels)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Prepare the data: Shape should be (batch_size, 1, num_channels, signal_length)\n",
    "data = torch.tensor(eeg_data, dtype=torch.float32)  # Data is (batch_size, num_channels, signal_length, 1)\n",
    "data = data.permute(0, 3, 1, 2)  # Reshape to (batch_size, 1, num_channels, signal_length)\n",
    "\n",
    "labels = torch.tensor(matching_df['label'][:count], dtype=torch.long)  # Labels as long tensor\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the ResNet2D model class\n",
    "class ResNet2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet2D, self).__init__()\n",
    "        # Load a pretrained ResNet (we'll modify it for EEG input)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Adjust the first convolutional layer to accept 1 input channel (from EEG)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Change the final fully connected layer to match the number of classes (normal vs abnormal)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 2)  # 2 classes (normal and abnormal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = ResNet2D()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming `dataloader` is defined and contains the training data\n",
    "# Split the data into train and validation (here, we use a simple split for illustration)\n",
    "train_size = int(0.8 * len(dataloader.dataset))\n",
    "val_size = len(dataloader.dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataloader.dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 3\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for this batch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch + 1}] Average Loss: {avg_loss:.3f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "\n",
    "    print(f\"[Epoch {epoch + 1}] Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Plot sensitivity and specificity over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), sensitivity_list, label='Sensitivity', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), specificity_list, label='Specificity', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sensitivity and Specificity Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
