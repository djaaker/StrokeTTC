{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 1, this kinda went nowhere, but it's still here for documentation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you prepare your EEG data for input into a ResNet model, we can go through the steps of loading the .edf files, preprocessing the data, and setting up the format for input into ResNet. Here's an outline of the process:\n",
    "\n",
    "1. **Load .edf files**: We'll use the `pyEDFlib` library to load the EEG data.\n",
    "2. **Preprocess the EEG data**: This may involve normalization or other preprocessing specific to EEG signals.\n",
    "3. **Label encoding**: We'll ensure that the labels (normal or abnormal) are mapped into numerical values.\n",
    "4. **Reshape data for ResNet**: ResNet typically expects input in a specific shape (e.g., (batch_size, height, width, channels)), so we'll format the EEG data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths to your 10 .edf files\n",
    "file_paths = ['file1.edf', 'file2.edf', 'file3.edf', 'file4.edf', 'file5.edf',\n",
    "              'file6.edf', 'file7.edf', 'file8.edf', 'file9.edf', 'file10.edf']\n",
    "\n",
    "# Labels for the files: 'normal' or 'abnormal'\n",
    "labels = ['normal', 'abnormal', 'normal', 'normal', 'abnormal', \n",
    "          'abnormal', 'normal', 'abnormal', 'normal', 'abnormal']\n",
    "\n",
    "# Prepare label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Function to load and preprocess each .edf file\n",
    "def load_and_preprocess_edf(file_path):\n",
    "    f = pyedflib.EdfReader(file_path)\n",
    "    num_signals = f.signals_in_file\n",
    "    signal_data = np.zeros((num_signals, f.getNSamples()[0]))\n",
    "\n",
    "    for i in range(num_signals):\n",
    "        signal_data[i, :] = f.readSignal(i)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    # Preprocess data (e.g., normalization)\n",
    "    # Here we simply normalize to 0-1 range as an example\n",
    "    signal_data = (signal_data - np.min(signal_data)) / (np.max(signal_data) - np.min(signal_data))\n",
    "    \n",
    "    return signal_data\n",
    "\n",
    "# Load and preprocess all the files\n",
    "eeg_data = []\n",
    "for path in file_paths:\n",
    "    eeg_data.append(load_and_preprocess_edf(path))\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "eeg_data = np.array(eeg_data)\n",
    "\n",
    "# Reshape data to fit ResNet input shape (assuming 1D signals, we can reshape to (samples, height, width, channels))\n",
    "# ResNet often takes 2D images, so you might need to adapt depending on your input size\n",
    "eeg_data = np.expand_dims(eeg_data, axis=-1)  # Add channel dimension\n",
    "\n",
    "print(\"EEG data shape:\", eeg_data.shape)\n",
    "print(\"Labels shape:\", categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Steps:\n",
    "1. **Loading .edf files**: We use `pyedflib` to load the EEG signals.\n",
    "2. **Preprocessing**: The data is normalized, but you can modify the preprocessing step based on your needs.\n",
    "3. **Label Encoding**: The labels (normal or abnormal) are encoded into categorical values for training.\n",
    "4. **ResNet Input Shape**: We ensure the EEG data is reshaped to the required input dimensions for ResNet (e.g., adding a channel dimension).\n",
    "\n",
    "Once the data is preprocessed, you can split it into training and test sets and feed it into the ResNet model.\n",
    "\n",
    "Let me know if you'd like to adjust anything, like the preprocessing or data augmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2 with pickle formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/abnormal/aaaaaaaq_s004_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/normal/aaaaaabn_s005_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/abnormal/aaaaaacq_s008_t001.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/abnormal/aaaaaacq_s009_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/normal/aaaaaaat_s002_t001.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/abnormal/aaaaaaav_s004_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/normal/aaaaaaff_s002_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/abnormal/aaaaaaaq_s005_t001.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/normal/aaaaaalk_s002_t000.edf: unpickling stack underflow\n",
      "Error loading /Users/User/Downloads/TUH EEG Corpus random files/normal/aaaaaama_s002_t000.edf: unpickling stack underflow\n",
      "Batch 1 is empty due to loading errors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset class to handle loading and processing of .edf files\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data_folder, label):\n",
    "        self.data_folder = data_folder\n",
    "        self.label = label\n",
    "        self.file_list = [f for f in os.listdir(data_folder) if f.endswith('.edf')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_folder, self.file_list[idx])\n",
    "        try:\n",
    "            # Load EEG data from .edf file using pickle (or any other preferred method)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data_pkl = pkl.load(f)\n",
    "                signals = data_pkl['RAW_DATA'][0]  # Extract EEG signals from the pickled data\n",
    "            return signals, self.label\n",
    "        except (pkl.UnpicklingError, KeyError) as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Prepare data loader function\n",
    "def prepare_dataloaders(normal_folder, abnormal_folder, batch_size=16):\n",
    "    normal_data = EEGDataset(normal_folder, label=0)\n",
    "    abnormal_data = EEGDataset(abnormal_folder, label=1)\n",
    "\n",
    "    # Combine datasets\n",
    "    dataset = torch.utils.data.ConcatDataset([normal_data, abnormal_data])\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# Custom collate function to handle None values\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    if len(batch) == 0:\n",
    "        return torch.Tensor(), torch.Tensor()\n",
    "    data, labels = zip(*batch)\n",
    "    return torch.stack([torch.Tensor(d) for d in data]), torch.Tensor(labels)\n",
    "\n",
    "# Define paths to normal and abnormal folders\n",
    "normal_folder = '/Users/User/Downloads/TUH EEG Corpus random files/normal'\n",
    "abnormal_folder = '/Users/User/Downloads/TUH EEG Corpus random files/abnormal'\n",
    "\n",
    "# Prepare the data loaders\n",
    "data_loader = prepare_dataloaders(normal_folder, abnormal_folder)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for batch_idx, (data, labels) in enumerate(data_loader):\n",
    "    if data.size(0) == 0:\n",
    "        print(f\"Batch {batch_idx + 1} is empty due to loading errors.\")\n",
    "        continue\n",
    "    print(f\"Batch {batch_idx + 1} - Data Shape: {data.shape}, Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 3 using pyedflib (this one worked!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note here, this isn't going to run for you straight away because for the sake of time I pulled the 10 data files from my downloads folder because pulling it from box wasn't working for me\n",
    "\n",
    "So either change the path to wherever the folder is for you (because it is in our shared box folder, and it's called TUH EEG Corpus random files), or figure out how to properly call it from box for me, because eventually we need a way to call all the data from some shared location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum signal length found: 1184\n",
      "Maximum number of channels found: 36\n",
      "Data shape: (10, 36, 1184, 1)\n",
      "Labels shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "\n",
    "# Function to find the length of the shortest signal and the maximum number of channels\n",
    "def find_min_length_and_max_channels(folder_paths):\n",
    "    min_length = float('inf')\n",
    "    max_channels = 0\n",
    "    \n",
    "    for folder in folder_paths:\n",
    "        for file_name in os.listdir(folder):\n",
    "            if file_name.endswith('.edf'):\n",
    "                file_path = os.path.join(folder, file_name)\n",
    "                with pyedflib.EdfReader(file_path) as f:\n",
    "                    n_signals = f.signals_in_file\n",
    "                    if n_signals > max_channels:\n",
    "                        max_channels = n_signals  # Track maximum number of channels\n",
    "                    for i in range(n_signals):\n",
    "                        signal_length = len(f.readSignal(i))\n",
    "                        if signal_length < min_length:\n",
    "                            min_length = signal_length  # Update if a smaller signal length is found\n",
    "    \n",
    "    return min_length, max_channels\n",
    "\n",
    "# Function to load signals from the .edf files and pad if necessary\n",
    "def load_edf_signals(file_path, target_length, target_channels):\n",
    "    with pyedflib.EdfReader(file_path) as f:\n",
    "        n_signals = f.signals_in_file\n",
    "        signal_data = []\n",
    "        for i in range(n_signals):\n",
    "            signal = f.readSignal(i)\n",
    "            truncated_signal = signal[:target_length]  # Truncate signal to target length\n",
    "            signal_data.append(truncated_signal)\n",
    "        \n",
    "        # Pad channels with zeros if fewer than the target number of channels\n",
    "        while len(signal_data) < target_channels:\n",
    "            signal_data.append(np.zeros(target_length))\n",
    "        \n",
    "        signal_data = np.array(signal_data)  # Convert list to numpy array for consistency\n",
    "    return signal_data\n",
    "\n",
    "# Prepare data and labels function\n",
    "def prepare_data(normal_folder, abnormal_folder, target_length, target_channels):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load normal files\n",
    "    for file_name in os.listdir(normal_folder):\n",
    "        if file_name.endswith('.edf'):\n",
    "            file_path = os.path.join(normal_folder, file_name)\n",
    "            signal_data = load_edf_signals(file_path, target_length, target_channels)\n",
    "            data.append(signal_data)\n",
    "            labels.append(0)  # Label 0 for normal\n",
    "    \n",
    "    # Load abnormal files\n",
    "    for file_name in os.listdir(abnormal_folder):\n",
    "        if file_name.endswith('.edf'):\n",
    "            file_path = os.path.join(abnormal_folder, file_name)\n",
    "            signal_data = load_edf_signals(file_path, target_length, target_channels)\n",
    "            data.append(signal_data)\n",
    "            labels.append(1)  # Label 1 for abnormal\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Paths to the folders\n",
    "normal_folder = '/Users/User/Downloads/BME Senior Design/BME 489/TUH EEG Corpus random files/normal'\n",
    "abnormal_folder = '/Users/User/Downloads/BME Senior Design/BME 489/TUH EEG Corpus random files/abnormal'\n",
    "\n",
    "# Find the minimum signal length and maximum number of channels across both folders\n",
    "folders = [normal_folder, abnormal_folder]\n",
    "min_signal_length, max_channels = find_min_length_and_max_channels(folders)\n",
    "print(f\"Minimum signal length found: {min_signal_length}\")\n",
    "print(f\"Maximum number of channels found: {max_channels}\")\n",
    "\n",
    "# Load and prepare the dataset with the minimum signal length and maximum channels\n",
    "data, labels = prepare_data(normal_folder, abnormal_folder, min_signal_length, max_channels)\n",
    "\n",
    "# Reshape data for ResNet (ResNet generally expects input shape of (n_samples, height, width, channels))\n",
    "# Let's assume the EEG data is multichannel, so reshape accordingly\n",
    "\n",
    "# UPDATE THE LABELS WITH THE KNOWN LABELS \n",
    "\n",
    "# THIS IS WHERE WE CAN UPLOAD PROCESSED DATA\n",
    "data = # max's dataframe (convert to numpy array)\n",
    "data = np.expand_dims(data, axis=-1)  # Add an extra dimension for channels if needed\n",
    "\n",
    "# Check the shapes of data and labels\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Prepare the data: Shape should be (batch_size, 1, num_channels, signal_length)\n",
    "data = torch.tensor(data, dtype=torch.float32)  # Data is (batch_size, num_channels, signal_length, 1)\n",
    "data = data.permute(0, 3, 1, 2)  # Reshape to (batch_size, 1, num_channels, signal_length)\n",
    "\n",
    "labels = torch.tensor(labels, dtype=torch.long)  # Labels as long tensor\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Data: tensor([[[[ -6.2526, -11.7458,  -9.4569,  ...,  16.9407,  15.8726,  18.7718],\n",
      "          [-23.1898, -19.2226, -13.2716,  ...,  12.6683,   9.6165,  16.7882],\n",
      "          [ 12.0579,   9.1588,   9.4639,  ...,   5.0389,   2.2923,   5.0389],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ..., 266.9000, 266.9000, 266.9000],\n",
      "          [  0.0000,   0.0000,  20.0000,  ...,   0.3000,   0.3000,   0.3000],\n",
      "          [  0.0000,   0.0000,  33.2000,  ...,  96.6000,  96.6000,  96.6000]]],\n",
      "\n",
      "\n",
      "        [[[  0.3087,   2.7501,   8.7010,  ..., -44.2470, -47.2987, -54.6229],\n",
      "          [  3.5130,   4.2760,   9.6165,  ..., -48.8246, -50.6556, -58.7428],\n",
      "          [-21.0536, -18.4596, -17.2389,  ..., -12.2035, -13.5768, -20.4433],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]])\n",
      "Labels: tensor([0, 1])\n",
      "\n",
      "Batch 2:\n",
      "Data: tensor([[[[ 4.9077e+03,  4.9193e+03,  4.9060e+03,  ..., -1.4648e+02,\n",
      "           -1.4694e+02, -1.4709e+02],\n",
      "          [ 4.7229e+03,  4.7151e+03,  4.7433e+03,  ..., -2.0614e+02,\n",
      "           -2.0630e+02, -2.0675e+02],\n",
      "          [ 4.9081e+03,  4.9184e+03,  4.9068e+03,  ..., -1.1169e+02,\n",
      "           -1.1123e+02, -1.1139e+02],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.9130e+01, -4.6994e+01, -4.6841e+01,  ..., -6.4846e+01,\n",
      "           -6.6677e+01, -6.8051e+01],\n",
      "          [-4.7451e+01, -4.3179e+01, -4.3179e+01,  ..., -5.7522e+01,\n",
      "           -5.7217e+01, -5.7217e+01],\n",
      "          [-4.2111e+01, -4.1958e+01, -4.2111e+01,  ..., -5.0350e+01,\n",
      "           -5.4623e+01, -6.1947e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.7900e+01,\n",
      "            7.7900e+01,  7.7900e+01],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  8.0000e-01,\n",
      "            8.0000e-01,  8.0000e-01],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  5.9900e+01,\n",
      "            5.9900e+01,  5.9900e+01]]]])\n",
      "Labels: tensor([1, 0])\n",
      "\n",
      "Batch 3:\n",
      "Data: tensor([[[[ 48.6790,  44.1014,  42.4229,  ...,  12.0579,   5.9544,   6.1070],\n",
      "          [ 47.4583,  43.3384,  41.0496,  ...,   9.6165,   5.9544,   7.0225],\n",
      "          [-26.6994, -22.7321, -25.1735,  ...,  13.4312,  25.1805,  20.6028],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]],\n",
      "\n",
      "\n",
      "        [[[  7.3277,  14.9571,  21.3658,  ..., -29.2934, -45.0099, -27.4623],\n",
      "          [  1.3768, -26.6994,  28.6900,  ..., -13.2716, -45.6202, -23.8002],\n",
      "          [  7.4803,  21.0606,  16.7882,  ...,   7.7855,  -4.1164,  -1.8276],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]])\n",
      "Labels: tensor([0, 1])\n",
      "\n",
      "Batch 4:\n",
      "Data: tensor([[[[ 45.4747,  47.4583,  47.3057,  ...,  40.5918,  40.1341,  38.9134],\n",
      "          [ 36.4720,  45.0169,  39.3711,  ...,  43.4910,  49.4419,  46.6954],\n",
      "          [ 50.5101,  51.5782,  52.3411,  ...,  16.3304,  16.1778,  12.6683],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ -4.7267,  -8.0837,  -9.1518,  ..., -22.8847, -20.9010, -17.5441],\n",
      "          [  6.8699,   2.1397,   4.4285,  ..., -28.5304, -26.6994, -22.7321],\n",
      "          [-13.5768, -10.2199, -15.1027,  ...,  -0.7594, -35.0917, -10.6776],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]])\n",
      "Labels: tensor([1, 1])\n",
      "\n",
      "Batch 5:\n",
      "Data: tensor([[[[ 3.0071e+03,  2.9935e+03,  2.8914e+03,  ...,  2.3960e+01,\n",
      "            3.5099e+01,  3.1437e+01],\n",
      "          [ 2.9668e+03,  2.9552e+03,  2.8474e+03,  ...,  3.8303e+01,\n",
      "            4.9137e+01,  4.9900e+01],\n",
      "          [ 2.9822e+03,  2.9684e+03,  2.8685e+03,  ...,  2.0908e+01,\n",
      "            3.1742e+01,  2.9453e+01],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2078e+00,  8.7010e+00,  7.9381e+00,  ..., -4.0890e+01,\n",
      "           -3.1582e+01, -3.1582e+01],\n",
      "          [ 7.3277e+00,  1.7246e+01,  1.7551e+01,  ..., -1.4645e+01,\n",
      "           -1.7239e+01, -1.5408e+01],\n",
      "          [-1.5224e+00,  5.8018e+00,  2.5975e+00,  ..., -1.2509e+01,\n",
      "           -8.8466e+00, -5.7948e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]]])\n",
      "Labels: tensor([0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through DataLoader and print each batch\n",
    "for batch_idx, (data_batch, labels_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Data:\", data_batch)\n",
    "    print(\"Labels:\", labels_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/User/anaconda3/envs/mne/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/User/anaconda3/envs/mne/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Average Loss: 1.371\n",
      "[Epoch 2] Average Loss: 1.367\n",
      "[Epoch 3] Average Loss: 2.500\n",
      "[Epoch 4] Average Loss: 0.901\n",
      "[Epoch 5] Average Loss: 0.678\n",
      "[Epoch 6] Average Loss: 0.784\n",
      "[Epoch 7] Average Loss: 0.729\n",
      "[Epoch 8] Average Loss: 0.642\n",
      "[Epoch 9] Average Loss: 0.658\n",
      "[Epoch 10] Average Loss: 0.681\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Use a prebuilt ResNet model and adjust the input for 2D data\n",
    "class ResNet2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet2D, self).__init__()\n",
    "        # Load a pretrained ResNet (we'll modify it for EEG input)\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        # Adjust the first convolutional layer to accept 1 input channel (from EEG)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # Change the final fully connected layer to match the number of classes (normal vs abnormal)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 2)  # 2 classes (normal and abnormal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNet2D()\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0  # Accumulate the loss for the epoch\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for this batch\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"[Epoch {epoch + 1}] Average Loss: {avg_loss:.3f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3648138051.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    rsync -auxvL --delete nedc-tuh-eeg@www.isip.piconepress.com:data/tuh_eeg/TEST .\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rsync -auxvL --delete nedc-tuh-eeg@www.isip.piconepress.com:data/tuh_eeg/TEST ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
