{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No label found for aaaaaagt\n",
      "No label found for dataset-tuh_task-binary_datatype-train_v6\n",
      "No label found for aaaaaqfm\n",
      "No label found for aaaaaqmo\n",
      "No label found for aaaaaqxr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaaaaac</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaag</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaaaaar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaaaaav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaaaabg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>aaaaaswc</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>aaaaatao</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>aaaaatba</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>aaaaatdq</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>aaaaatdt</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>618 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  label  test\n",
       "0    aaaaaaac      3     0\n",
       "1    aaaaaaag      3     0\n",
       "2    aaaaaaar      0     0\n",
       "3    aaaaaaav      0     0\n",
       "4    aaaaaabg      0     0\n",
       "..        ...    ...   ...\n",
       "613  aaaaaswc      3     1\n",
       "614  aaaaatao      0     1\n",
       "615  aaaaatba      0     1\n",
       "616  aaaaatdq      3     1\n",
       "617  aaaaatdt      3     1\n",
       "\n",
       "[618 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "trainPath = r\"D:\\v2.0.3\\edf\\train\"\n",
    "evalPath = r\"D:\\v2.0.3\\edf\\eval\"\n",
    "labelPath = r\"C:\\Users\\dalto\\Box Sync\\abnLabels.csv\"\n",
    "\n",
    "labelFrame = pd.read_csv(labelPath)\n",
    "\n",
    "labels = []\n",
    "names = []\n",
    "tests = []\n",
    "noLabelCnt = 0\n",
    "\n",
    "for split in enumerate([os.listdir(trainPath), os.listdir(evalPath)]):\n",
    "    test = split[0]\n",
    "\n",
    "    subName = split[1]\n",
    "\n",
    "    for sub in subName:\n",
    "        # if sub is in labelFrame\n",
    "        if len(labelFrame.loc[labelFrame['name'] == sub]) == 0:\n",
    "            print(\"No label found for\", sub)\n",
    "            noLabelCnt += 1\n",
    "        else:\n",
    "            labels.append(labelFrame.loc[labelFrame['name'] == sub, 'label'].values[0])\n",
    "            names.append(sub)\n",
    "            tests.append(test)\n",
    "\n",
    "df = pd.DataFrame({'name': names, 'label': labels, 'test': tests})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edf count: 4662\n",
      "Healthy count: 3489\n",
      "Seizure count: 1173\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# list all of the .edf files in each train and eval folder\n",
    "healthyEdfs = []  # List to store the paths of all .edf files\n",
    "seizEdfs = []  # List to store the paths of all .edf files\n",
    "edfLabels = []  # List to store the labels corresponding to each .edf file\n",
    "\n",
    "# Sample of the 'df' DataFrame (in your case, the DataFrame might already be available)\n",
    "# df = pd.read_csv('path/to/your_dataframe.csv')\n",
    "\n",
    "# Loop through each subject in the 'names' list\n",
    "for subject in names:\n",
    "    subject_path = os.path.join(trainPath, subject)\n",
    "    \n",
    "    # Find the label corresponding to the subject from the DataFrame\n",
    "    subject_label = df[df['name'] == subject]['label'].values[0]\n",
    "    \n",
    "    # Loop through all the date of recording folders (e.g., s001_2002)\n",
    "    if os.path.exists(subject_path):\n",
    "        for recording_date in os.listdir(subject_path):\n",
    "            recording_date_path = os.path.join(subject_path, recording_date)\n",
    "            \n",
    "            # Loop through all cap setup folders (e.g., 02_tcp_le)\n",
    "            if os.path.isdir(recording_date_path):\n",
    "                for cap_setup in os.listdir(recording_date_path):\n",
    "                    cap_setup_path = os.path.join(recording_date_path, cap_setup)\n",
    "                    \n",
    "                    # Find all .edf files in the final directory\n",
    "                    if os.path.isdir(cap_setup_path):\n",
    "                        edf_files_in_dir = glob.glob(os.path.join(cap_setup_path, \"*.edf\"))\n",
    "                        \n",
    "                        # Add each .edf file to the list, and also add its corresponding label\n",
    "                        for edf_file in edf_files_in_dir:\n",
    "                            if subject_label == 0:\n",
    "                                healthyEdfs.append(edf_file)\n",
    "                            else:\n",
    "                                seizEdfs.append(edf_file)\n",
    "\n",
    "# check for any duplicate .edf files\n",
    "healthyEdfs = list(set(healthyEdfs))\n",
    "seizEdfs = list(set(seizEdfs))\n",
    "edfFiles = healthyEdfs + seizEdfs\n",
    "\n",
    "# add the labels to the edf files\n",
    "edfLabels = [0] * len(healthyEdfs) + [3] * len(seizEdfs)\n",
    "\n",
    "# print the number of edf files\n",
    "print(\"Edf count:\", len(edfFiles))\n",
    "\n",
    "# print the number of subjects in each edfLabel class\n",
    "print(\"Healthy count:\", len([x for x in edfLabels if x == 0]))\n",
    "print(\"Seizure count:\", len([x for x in edfLabels if x == 3]))\n",
    "\n",
    "\n",
    "#### ONLY FOR EXPORTING TO CSV ####\n",
    "# # take everything after 'train\\' and add it to a new list\n",
    "# edfNames = [x.split('train\\\\')[1] for x in edfFiles]\n",
    "\n",
    "# # save the edfNames and edfLabels to a csv\n",
    "# edfDf = pd.DataFrame({'name': edfNames, 'label': edfLabels})\n",
    "# edfDf.to_csv('edfFiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired number of channels and fixed length of time points\n",
    "TARGET_CHANNELS = 40  # Number of channels you want to have in the final data\n",
    "TARGET_POINTS = 75000  # Fixed number of time points for each sample\n",
    "\n",
    "# Function to load and preprocess each .edf file\n",
    "def load_and_preprocess_edf(filePath, target_channels=TARGET_CHANNELS, target_points=TARGET_POINTS):\n",
    "    # Load the raw EEG data\n",
    "    RawEEGDataFile = mne.io.read_raw_edf(filePath, preload=True, verbose=False)\n",
    "    RawEEGDataFile.interpolate_bads()\n",
    "\n",
    "    # Get the raw data (channels × time)\n",
    "    data = RawEEGDataFile.get_data()\n",
    "\n",
    "    # Determine current number of channels\n",
    "    current_channels, current_points = data.shape\n",
    "\n",
    "    # Pad or truncate channels to make them equal to target_channels (e.g., 40)\n",
    "    if current_channels < target_channels:\n",
    "        # Pad with zeros if there are fewer channels than target_channels\n",
    "        padding = target_channels - current_channels\n",
    "        data = np.pad(data, ((0, padding), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        # Truncate channels if there are more than target_channels\n",
    "        data = data[:target_channels, :]\n",
    "\n",
    "    # Interpolate or resample data to ensure target_points are present\n",
    "    if current_points != target_points:\n",
    "        data = np.array([np.interp(np.linspace(0, current_points - 1, target_points), np.arange(current_points), data[ch, :]) for ch in range(target_channels)])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from D:\\v2.0.3\\edf\\train\\aaaaadns\\s004_2014\\01_tcp_ar\\aaaaadns_s004_t001.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 3249  =      0.000 ...    12.996 secs...\n",
      "Extracting EDF parameters from D:\\v2.0.3\\edf\\train\\aaaaanrb\\s005_2012\\01_tcp_ar\\aaaaanrb_s005_t005.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 150249  =      0.000 ...   600.996 secs...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.9 MiB for an array with shape (40, 75000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    102\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m    104\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# Get labels from edfLabels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mEEGDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Interpolate or compress each segment to match target_points\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segment\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_points:\n\u001b[1;32m---> 46\u001b[0m     segment \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Reshape segment to match the input dimensions required by the ResNet model\u001b[39;00m\n\u001b[0;32m     49\u001b[0m segment \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(segment, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add a batch dimension if needed\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 22.9 MiB for an array with shape (40, 75000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import mne  # assuming mne is used for EEG data loading\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gc\n",
    "\n",
    "# Cell 2: Define a dataset class to load EEG data in segments\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, eeg_file_paths, target_channels=40, target_points=75000, segment_length=25000):\n",
    "        self.eeg_file_paths = eeg_file_paths\n",
    "        self.target_channels = target_channels\n",
    "        self.target_points = target_points\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eeg_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.eeg_file_paths[idx]\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "        \n",
    "        # Apply preprocessing steps such as ICA or filtering here\n",
    "        data = raw.get_data()  # Shape: (channels, time_points)\n",
    "        \n",
    "        # Pad or trim channels to match target_channels\n",
    "        if data.shape[0] < self.target_channels:\n",
    "            padding = np.zeros((self.target_channels - data.shape[0], data.shape[1]))\n",
    "            data = np.vstack((data, padding))\n",
    "        elif data.shape[0] > self.target_channels:\n",
    "            data = data[:self.target_channels, :]\n",
    "        \n",
    "        # Split the data into segments to handle large files\n",
    "        segments = []\n",
    "        for start in range(0, data.shape[1], self.segment_length):\n",
    "            end = min(start + self.segment_length, data.shape[1])\n",
    "            segment = data[:, start:end]\n",
    "            \n",
    "            # Interpolate or compress each segment to match target_points\n",
    "            if segment.shape[1] != self.target_points:\n",
    "                segment = np.array([np.interp(np.linspace(0, 1, self.target_points), np.linspace(0, 1, segment.shape[1]), channel) for channel in segment])\n",
    "            \n",
    "            # Reshape segment to match the input dimensions required by the ResNet model\n",
    "            segment = np.expand_dims(segment, axis=0)  # Add a batch dimension if needed\n",
    "            segments.append(torch.tensor(segment, dtype=torch.float32))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "# Cell 3: List of all your EEG file paths\n",
    "eeg_files = edfFiles\n",
    "\n",
    "# Cell 4: Create a dataset and a dataloader for batch processing\n",
    "batch_size = 2  # Set batch size based on your system's memory\n",
    "segment_length = 5000  # Set segment length based on your system's memory\n",
    "\n",
    "dataset = EEGDataset(eeg_files, segment_length=segment_length)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Flatten the list of segments and create a new batch\n",
    "    segments = [segment for segments in batch for segment in segments]\n",
    "    return torch.stack(segments)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Cell 5: Define the ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Define a simple ResNet-like architecture\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet()\n",
    "\n",
    "# Cell 6: Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Cell 7: Training loop with batch processing\n",
    "num_epochs = 2  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs = batch\n",
    "        # Get labels from edfLabels\n",
    "        labels = edfLabels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Clear unused variables to free memory\n",
    "        del inputs, labels, outputs, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Explanation of Computational Efficiency\n",
    "# Increasing the batch size can make the training more computationally efficient, as it allows more data to be processed in parallel.\n",
    "# However, it also requires more memory. If your system has limited memory, a smaller batch size may be more practical to avoid crashes.\n",
    "# Increasing the segment length means more data is processed per segment, which can also improve efficiency.\n",
    "# However, larger segments require more memory, which may not be feasible on systems with limited resources.\n",
    "# Therefore, both batch size and segment length should be chosen carefully based on the available memory to balance efficiency and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
